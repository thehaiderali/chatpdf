Hereâ€™s a **clean and beginner-friendly README** for your project ğŸš€

---

# ğŸ“„ Chat with Your PDFs (Gemini + LangChain + FAISS)

This project is a **PDF Question Answering App** built with **Streamlit**, **Google Gemini API**, and **LangChain**.
It lets you upload PDF documents, process them into searchable chunks, and then ask natural language questions.
The app retrieves relevant sections from your documents and uses **Gemini LLM** to provide accurate answers.

---

## âš¡ Features

* ğŸ“¤ **Upload PDFs** via a simple Streamlit interface
* âœ‚ï¸ **Automatic text extraction & chunking** from PDFs
* ğŸ§  **Embeddings with Gemini** (`gemini-embedding-001`)
* ğŸ“š **Vector search with FAISS** to find relevant chunks
* ğŸ¤– **Gemini chat model** (`gemini-2.0-flash-001`) for answering questions
* ğŸ” View the retrieved context before the answer (transparency)

---

## ğŸ› ï¸ Tech Stack

* [Streamlit](https://streamlit.io/) â€“ UI framework
* [PyPDF2](https://pypi.org/project/pypdf2/) â€“ PDF text extraction
* [LangChain](https://www.langchain.com/) â€“ text splitting, vector stores, prompt templates
* [Google Gemini API](https://ai.google.dev/) â€“ embeddings + chat LLM
* [FAISS](https://faiss.ai/) â€“ vector similarity search

---

## ğŸš€ Setup Instructions

### 1. Clone the repository

```bash
git clone https://github.com/yourusername/chat-with-pdf-gemini.git
cd chat-with-pdf-gemini
```

### 2. Create a virtual environment

Using conda:

```bash
conda create -n myenv python=3.10 -y
conda activate myenv
```

Or using venv:

```bash
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Set up environment variables

Create a `.env` file in the project root:

```
GEMINI_API_KEY=your_google_gemini_api_key
```

> ğŸ”‘ You can get your API key from the [Google AI Studio](https://aistudio.google.com/).

### 5. Run the app

```bash
streamlit run app.py
```

---

## ğŸ“– Usage

1. Open the app in your browser (Streamlit will give you a local URL).
2. Upload one or more PDF files from the sidebar.
3. Click **Process PDFs** â†’ the app extracts text, chunks it, and stores embeddings in FAISS.
4. Ask a question in the text box.
5. The app retrieves the top relevant chunks and sends them to **Gemini** for answering.
6. Expand the **ğŸ“– Retrieved Context** section to see the exact passages used.

---

## ğŸ–¼ï¸ Example

* Upload: `machine_learning.pdf`
* Question: *"What is overfitting?"*
* Retrieved Context: shows chunks from the PDF explaining overfitting.
* Answer: *"Overfitting occurs when a model learns the training data too well, including noise, and fails to generalize to new data."*

---

## ğŸ§© Project Structure

```
.
â”œâ”€â”€ app.py          # Main Streamlit app
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env            # API key (not committed to git)
â””â”€â”€ vectorstore/    # Saved FAISS database
```

---

## âš ï¸ Notes

* Free Gemini API tier has **rate limits** â†’ heavy use may hit quota.
* `FAISS` saves embeddings locally â†’ delete `vectorstore/` to reset.
* This app answers **based only on your PDFs** (RAG pipeline).

---

## ğŸ“Œ Roadmap

* âœ… Basic single-turn QA
* â³ Add **chat history** for multi-turn conversations
* â³ Support **other file types** (DOCX, TXT)
* â³ Deploy online (e.g., Streamlit Cloud, Render, or with ngrok)

---

## ğŸ™Œ Acknowledgements

* [LangChain](https://www.langchain.com/)
* [Google Gemini](https://ai.google.dev/)
* [Streamlit](https://streamlit.io/)

---

ğŸ‘‰ With this, youâ€™ve got a working **Chat with PDF using Gemini** app.

---
